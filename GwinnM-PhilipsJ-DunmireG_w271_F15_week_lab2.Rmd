---
title: "Lab 2"
author: "Glenn (Ted) Dunmire"
date: "November 13, 2015"
output: html_document
---

##Question 1: Concepts

**_1.1 (1 points) Define the term stochastic process._**

A stochastic process is a statistical phenomenon that evolves over time according to probabilistic laws-- a stochastic process generates a single time series in the same way a random variable generates a single number.

**_1.2 (2 points) Define the term time series. What is the difference between a stochastic process and a time series?_**

A time series is a set of observations generated sequentially in time. A time series arises as the result of a stochastic process and is a realization of the stochastic process with specific values over a time period.

**_1.3 (3 points) In your own words, discuss the mean function, variance function, and measure of dependency structure in the context of time series (week 8 - 10) and compare them with those we studied in classical linear model (week 1 - 7)._**

Prevously, in the classic linear model, there was no concept of time when we discussed the mean. With a time series, we add the complexity of time. It no longer makes sense to just report a mean if the mean is changing over time. Now, the mean function is $\mu_x(t) = E[x_t] = \int_{-\infty}^{+\infty}x_tf_t(x_t)dx_t$. The expectation is over the set of all time series that could be generated by the stochastic process. If a process is stationary in the mean, that means the mean is constant and we can drop the time parameter and write the mean as $\mu$, similar to what we studied in a classic linear model.

The variance function is $\sigma^2_x(t) = E(x_t - \mu_x(t))^2 = \int_{-\infty}^{+\infty} (x_t-\mu_x(t))^2f_t(x_t)dx_t$. Variance also may depend on time, but it's impossible to estimate a separate variance because we only have a single time series. For this reason, it's popular to assume that the series is stationary in variance meaning that the variance function is constant across time.

In classical models, we measure linear dependency using correlation or covariance. In a time series, we are interested in dependency between different random variables in the same series. In order to do this, we use autocovariance and autocorrelation. Typically, we think of autocovariance and autocorrelation being a function of the lag in a time series. 

**_1.4 (2 points) Define strict and weak statonarity_**

Strict stationarity is when the joint distributions for all time periods are the same. Formally it is if 
$$F(x_t1, . . ., x_tn)$$ and $$F(x_t1+m, . . ., x_tn+m)$$ are equal for all (t1, . . ., tn) and m. 

Weak stationarity is when a time series is stationary in mean and variance and if its autocovariance depends on the lag (k) and can be written as $$\gamma^{k}$$

**_1.5 (3 points) Give an example of a time series in real life. Describe the series. Evaluate (not empirical work is needed) whether or not the series can be modeled using the class of autoregressive models?_**

An example of a time series in real life would be monthly bookings at Walt Disney World Resort from 1980-2015. This time series would fluctuate seasonally and would likely have an upward trend, especially given more rooms are added over time. This time series is not likely to be stationary in the mean, and thus wouldn't be good for an autoregressive model without transformations. To use autoregressive techniques on a series like this, the series would first need to be made stationary by removing trends and seasonality.

**_1.6 (4 points) In your own words, define and describe partial autocorrelation function (PACF). Why is it not enough just to autocorrelation functon (ACF) to capture the dependency of a series?_**

The partial autocorrelation function partials out variance explained due to shorter lags. For example, a partial autocorrelation function for the 3rd lag is controlling for the 2nd lag and 1st lag. You must look at the partial autocorrelation function to capture the unique dependency of the series attributable to only the lag of interest and not the lags before it.

## Question 2
**_Determine if each of the following models is stationary_**

**2.1** $z_t=0.95z_{t-1}+\omega_t$

Reorganize in terms of backward operator

$\omega_t = 1 - 0.95B$

set $\omega_t$ equal to 0

$0 = 1 - 0.95B$

Solve for B

$B = 1/0.95 = 1.05$

As the root is greater than 1, the AR model is stationary

Alternatively you can use the polyroot function to determine the roots:

```{r}
polyroot(c(1,-.95))
```

This gives the same result as above.

**2.2** $z_t=0.8z_{t-1}+0.3z_{t-2}+\omega_t$

Reorganize in terms of backward operator

$\omega_t = 1 - 0.8B -0.3B^2$

Use polyroot

```{r}
polyroot(c(1,-0.8,-0.3))
```

As one root is not greater than 1 in absolute value, the AR model is not stationary

**2.3** $z_t=-0.5z_{t-1}+0.5z_{t-2}+\omega_t$

Reorganize in terms of backward operator

$\omega_t = 1 + 0.5B -0.5B^2$

Use polyroot

```{r}
polyroot(c(1,0.5,-0.5))
```

As one root is not greater than 1 in absolute value, the AR model is not stationary

**2.4** $z_t=z_{t-1}+0.4z_{t-2}+\omega_t$

Reorganize in terms of backward operator

$\omega_t = 1 - B -0.4B^2$

Use polyroot

```{r}
polyroot(c(1,-1,-0.4))
```

As one root is not greater than 1 in absolute value, the AR model is not stationary
 
**2.5** $z_t=-0.5z_{t-1}-0.25z_{t-2}+\omega_t$

Reorganize in terms of backward operator

$\omega_t = 1 + 0.5B +0.25B^2$

Use polyroot

```{r}
polyroot(c(1.0,0.5,0.25))
```

As both roots are greater than 1 in absolute value, the AR model is stationary

## Question 3

**_3.1 Load the series series1.csv_**

```{r}
#Read Series Data

series <- read.table("series.csv", quote="\"", comment.char="")
#series <- read.table("//vivica/Documents/MIDS/W271/w271_F15_week10_lab2/series.csv", quote="\"", comment.char="")

#Reference libraries
library(ggplot2)
library(gridExtra)
library(grid)
library(car)
```

**_3.2 Describe the basic structure of the data and provide summary statistics of the series_**

```{r}
summ = summary(series)
sd = sd(series$V1)
head(series)
```

There are 75 observations in the series, of 1 variable.  The mean of the series is `r summ[4]` while the standard deviation is `r sd`.  The minimum and maximum of the series are `r summ [1]` and `r summ [6]`, respectively. The 25th, 50th and 75th quantiles are `r summ[2]`, `r summ[3]` and `r summ[5]`, respectively. We have displayed the first 5 observations using the 'head' command to show in general what the series looks like. 

**_3.3 Plot histogram and time-series plot of the series.  Describe the patterns exhibited in histogram and time-series plot.  For time series analysis, is it sufficient to use only histogram to describe a series?_**

```{r, echo=FALSE}

#Sequence data from first observation to last observation
ind = seq(from = 1, to = 75)
df = data.frame(ind,series)

#Create time series graph
ts = ggplot(df, aes(x=ind, y=V1)) + geom_line() + ggtitle("Time Series")+ scale_x_continuous("Series") + scale_y_continuous("Value") 

#Create histogram of time series (ignoring time)
hist = ggplot(df, aes(df$V1)) + geom_histogram()+ ggtitle("Histogram")+ scale_x_continuous("Value")+ scale_y_continuous("Frequency")

#Arrange graphs next to each other and provide overarching title
grid.arrange(ts, hist, ncol=2,nrow=1, top = textGrob("Time Series and Histogram from Lab 2 Question 3",gp=gpar(fontsize=12,font=2)))
```

The time series shows an increasing trend from 0 through 25 and after that there is a decreasing trend until the end of the dataset. However, around time period 39 there is a large drop and then a significant spike persisting through time period 45 (approximately). Then the series continues to trend downward. There is seasonality throughout the entire dataset, exhibited by frequent peaks and troughs. 

The distribution is clearly not normal and appears to be platykurtic, as there are many frequent points around the mean. 

In general with time series it is not sufficient to examine only the histogram. The histogram will tell us the number of times a particular value occurs in the dataset. However, quite often with time series we are interested in the trend or changes in the observations through time. A histogram does not give us any insight into how the values will change over time, for example, in detecting seasonality. A histogram is thus certainly valuable, but not sufficient. 

**_3.4 Plot the ACF and PACF of the series.  Describe the patterns exhibited in the ACF and PACF_**

```{r}

#Plot the ACF of the time series
acf(series, main="")
title("Autocorrelation of Time Series")

#Plot the PACF of the time series
pacf(series, main="")
title("Partial Autocorrelation of Time Series")

```


The autocorrelation of the time series indicates that the series is highly dependent on its previous lag at a statistically significant level through the 8th lag. The autocorrelation function does not fluctuate around zero, which probably indicates that the parameters of the series are positive. Based on how long it takes for the function to decay, this suggests there is a moderate level of persistence in the time series. 

The partial autocorrelation of the time series indicates that the series is only significantly dependent on previous values through the second lag. This means that after controlling for the shorter lags, only the first two lags remain statistically significant.

**_3.5 Esimate the series using the maximum likelihood method option of ar() functions_**

```{r}
arfit <- ar(df$V1, method = "mle")
arfit
```

**_3.6 Report the estimated AR parameters, the order of the model and standard errors_**

The order of the model is `r arfit$order` while the estimated parameters are `r arfit$ar`.  The standard errors are `r diag(sqrt(arfit$asy.var))`

**_3.7 Estimate the series using the Ordinary Least Square option of ar() functions_**

```{r}
arfit <- ar(df$V1, method = "ols")
arfit
```

**_3.8 Report the estimated AR parameters, the order of the model and standard errors_**

The order of the model is `r arfit$order` while the estimated parameters are `r arfit$ar`.  The standard errors are `r arfit$asy.se$ar`

**_3.9 Estimate the series using the Yule-Walker Equations option of ar() functions_**

```{r}
arfit <- ar(df$V1, method = "yw")
arfit
```

**_3.10 Report the estimated AR parameters, the order of the model and standard errors_**

The order of the model is `r arfit$order` while the estimated parameters are `r arfit$ar`.  The standard errors are `r diag(sqrt(arfit$asy.var))`

**_3.11 Are these estimates the same?  If so, derive the forumla to justify your answer.  If not, please explain.  How does the function ar() choose the best AR model_**

The order of these models are not necessarily the same. The MLE and Yule-Walker methods give a 2 order model, while the OLS method gives a 4 order model. That being said, the standard errors of the estimates themselves tend to be large, making the confidence interval fall in the range of +/- about .20, making it a range of about .40. We cannot say these estimates are significantly different from one another after calculating the z-scores between parameters: 

z = (P1 - P2) / √(seP1^2 + seP2^2)

## Question 4

**_4.1 Simulate a time series of length 100 for the following model.  Name the series x._**

$$x_t=\frac{5}{6}x_{t-1}-\frac{1}{6}x_{t-2}+\omega_t$$


```{r}
set.seed(100)
x <- arima.sim(n = 100, list(ar = c(5/6, -1/6), ma=0))
plot.ts(x, xlab = "Simulated time period", ylab = "Simulated Value", main = "Simulated Time Series Model")
hist(x, xlab = "Simulated Value", main = "Histogram of Simulated Time Series")
```

**_4.2 Plot the correlogram and partial correlogram for the simluation series.  Comment on the plots_**

```{r}
par(mfrow = c(2,2))
#Autocorrelation Function
acf(x, main="")
title("Autocorrelation of Time Series")

#Partial Autocorrelation Function
pacf(x, main="")
title("Partial Autocorrelation of Time Series")
```

The autocorrelation of the time series indicates that the series is highly dependent on its previous lag at a statistically significant level through the 2nd lag. The switching signs in the autocorrelation suggests a negative parameter (which we know is correct from the model given). 

The partial autocorrelation of the time series indicates that the series is only significantly dependent on previous values through the first lag. This means that after controlling for the shorter lags, only the first lag remains statistically significant. This would suggest an AR(1) model would be appropriate to estimate this series. 

**_4.3 Estimate an AR model for this simulated series.  Report the estimated AR parameters, standard errors and the order of the AR model_**

```{r}
arfit <- ar(x, method = "mle")
```

The order of the model is `r arfit$order` while the estimated parameter is `r arfit$ar`.  The standard error is `r sqrt(arfit$asy.var.coef)`

**_4.4 Construct a 95% confident intervals for the parameter estimates of the estimated model.  Do the 'true' mode parameters fall within the confidence intervals?  Explain the 95% confidence intervals in this context_**

```{r}
CI <- arfit$ar + c(-2, 2) * sqrt(arfit$asy.var.coef)
```

This confidence interval for the estimated parameter is `r CI`. To recall, the true parameters of this model are `r 5/6` and `r -1/6`. Neither of these parameters are contained in the confidence interval obtained for the simulated AR(1) model. In the classic linear model, we check if the confidence contains 0 because if the confidence interval does contain 0 we know that the test statistic will not be statisically significant (assuming a 0.05 level). In this case, we are interested in determining if our model's estimated parameter contains the true parameter, to see how well our model estimates the true series. However, in this case our model does not contain the true parameters so we would conclude (at the 0.05 level) our models estimate is statisitcally significantly different from the true parameter. 

**_4.5 Is the estimated model stationary or not station?_**

```{r}
polyroot(c(1, -arfit$ar))
```

The root is greater than 1 in absolute value, therefore this model is stationary. 

**_4.6 Plot the correlogram of the residuals of the estimated model.  Comment on the plot_**

```{r}
resids <- arfit$resid[2:100] #subsetting to remove NA
par(mfrow = c(2,2))
#Autocorrelation Function of Residuals
acf(resids, main="Autocorrelation of Time Series Residuals")

#Partial Autocorrelation Function of Residuals
pacf(resids, main = "Partial Autocorrelation of Time Series Residuals")

#Time Series of Residuals
plot.ts(resids, main="Simulated Series Residuals",col="blue", ylab="Values", xlab="Simulated Time #Period")

#White noise for comparison
plot(rnorm(100), type="l", main="Gaussian White Noise")
```

The autocorrelation function does not really show any autocorrelation. 

At a glance, it's a little hard to tell the white noise and the residuals apart. The simulated residuals display more flucuations than the white noise, but there is not an obvious trend. Based on that, and the lack of autocorrelation from above, it is probably reasonable to say the evidence supports the hypothesis that the residuals resemble white noise. 

```{r}
#Histogram of Residuals
hist(resids, main = "Histogram of TS Residuals")

#QQ Plot of residuals
qqPlot(resids, main="QQ Plot of Residuals", ylab="Residuals")
```

The histogram does not look particularly normally distributed. However, the QQ plot is mostly normal, following the line with some exceptions. The QQ plot shows a bit of a trend in the residuals, as values fluctuate below and above the y = x line especially in the positive direction. This is probably enough evidence to be cautious about concluding the residuals are normally distributed. 
